{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually explore Arevalo2023 pipeline results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icb/tim.treis/miniconda/envs/batchcp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import pyarrow.parquet as pq\n",
    "from tqdm.contrib.concurrent import thread_map\n",
    "import anndata as ad\n",
    "from scvi.external import SysVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### io.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_anndata(parquet_path):\n",
    "    meta, feats, features = split_parquet(parquet_path)\n",
    "    meta.index = meta.index.astype(str)\n",
    "    adata = ad.AnnData(feats, meta)\n",
    "    adata.var_names = features\n",
    "    return adata\n",
    "\n",
    "\n",
    "def split_parquet(dframe_path,\n",
    "                  features=None) -> tuple[pd.DataFrame, np.ndarray, list[str]]:\n",
    "    dframe = pd.read_parquet(dframe_path)\n",
    "    if features is None:\n",
    "        features = find_feat_cols(dframe)\n",
    "    vals = np.empty((len(dframe), len(features)), dtype=np.float32)\n",
    "    for i, c in enumerate(features):\n",
    "        vals[:, i] = dframe[c]\n",
    "    meta = dframe[find_meta_cols(dframe)].copy()\n",
    "    return meta, vals, features\n",
    "\n",
    "\n",
    "def merge_parquet(meta, vals, features, output_path) -> None:\n",
    "    '''Save the data in a parquet file resetting the index'''\n",
    "    dframe = pd.DataFrame(vals, columns=features)\n",
    "    for c in meta:\n",
    "        dframe[c] = meta[c].reset_index(drop=True)\n",
    "    dframe.to_parquet(output_path)\n",
    "\n",
    "\n",
    "def get_num_rows(path) -> int:\n",
    "    '''Count the number of rows in a parquet file'''\n",
    "    with pq.ParquetFile(path) as file:\n",
    "        return file.metadata.num_rows\n",
    "\n",
    "\n",
    "def prealloc_params(sources, plate_types):\n",
    "    '''\n",
    "    Get a list of paths to the parquet files and the corresponding slices\n",
    "    for further concatenation\n",
    "    '''\n",
    "    meta = load_metadata(sources, plate_types)\n",
    "    paths = (meta[['Metadata_Source', 'Metadata_Batch',\n",
    "                   'Metadata_Plate']].drop_duplicates().apply(build_path,\n",
    "                                                              axis=1)).values\n",
    "\n",
    "    counts = thread_map(get_num_rows, paths, leave=False, desc='counts')\n",
    "    slices = np.zeros((len(paths), 2), dtype=int)\n",
    "    slices[:, 1] = np.cumsum(counts)\n",
    "    slices[1:, 0] = slices[:-1, 1]\n",
    "    return paths, slices\n",
    "\n",
    "\n",
    "def load_data(sources, plate_types):\n",
    "    '''Load all plates given the params'''\n",
    "    paths, slices = prealloc_params(sources, plate_types)\n",
    "    total = slices[-1, 1]\n",
    "\n",
    "    with pq.ParquetFile(paths[0]) as f:\n",
    "        meta_cols = find_meta_cols(f.schema.names)\n",
    "        feat_cols = find_feat_cols(f.schema.names)\n",
    "    meta = np.empty([total, len(meta_cols)], dtype='|S128')\n",
    "    feats = np.empty([total, len(feat_cols)], dtype=np.float32)\n",
    "\n",
    "    def read_parquet(params):\n",
    "        path, start, end = params\n",
    "        df = pd.read_parquet(path)\n",
    "        meta[start:end] = df[meta_cols].values\n",
    "        feats[start:end] = df[feat_cols].values\n",
    "\n",
    "    params = np.concatenate([paths[:, None], slices], axis=1)\n",
    "    thread_map(read_parquet, params)\n",
    "\n",
    "    meta = pd.DataFrame(data=meta.astype(str),\n",
    "                        columns=meta_cols,\n",
    "                        dtype='category')\n",
    "    dframe = pd.DataFrame(columns=feat_cols, data=feats)\n",
    "    for col in meta_cols:\n",
    "        dframe[col] = meta[col]\n",
    "    return dframe\n",
    "\n",
    "\n",
    "def add_pert_type(meta: pd.DataFrame, col: str = 'Metadata_PertType'):\n",
    "    meta[col] = 'trt'\n",
    "    meta.loc[~meta['Metadata_JCP2022'].str.startswith('JCP'), col] = 'poscon'\n",
    "    meta.loc[meta['Metadata_JCP2022'] == 'DMSO', col] = 'negcon'\n",
    "    meta[col] = meta[col].astype('category')\n",
    "\n",
    "\n",
    "def add_row_col(meta: pd.DataFrame):\n",
    "    '''Add Metadata_Row and Metadata_Column to the DataFrame'''\n",
    "    well_regex = r'^(?P<row>[a-zA-Z]{1,2})(?P<column>[0-9]{1,2})$'\n",
    "    position = meta['Metadata_Well'].str.extract(well_regex)\n",
    "    meta['Metadata_Row'] = position['row'].astype('category')\n",
    "    meta['Metadata_Column'] = position['column'].astype('category')\n",
    "\n",
    "def add_microscopy_info(meta: pd.DataFrame):\n",
    "    configs = meta['Metadata_Source'].map(MICRO_CONFIG).astype('category')\n",
    "    meta['Metadata_Microscope'] = configs\n",
    "\n",
    "def write_parquet(sources, plate_types, output_file):\n",
    "    '''Write the parquet dataset given the params'''\n",
    "    dframe = load_data(sources, plate_types)\n",
    "    # Efficient merge\n",
    "    meta = load_metadata(sources, plate_types)\n",
    "    add_pert_type(meta)\n",
    "    add_row_col(meta)\n",
    "    add_microscopy_info(meta)\n",
    "    foreign_key = ['Metadata_Source', 'Metadata_Plate', 'Metadata_Well']\n",
    "    meta = dframe[foreign_key].merge(meta, on=foreign_key, how='left')\n",
    "    for c in meta:\n",
    "        dframe[c] = meta[c].astype('category')\n",
    "    # Dropping samples with no metadata\n",
    "    dframe.dropna(subset=['Metadata_JCP2022'], inplace=True)\n",
    "    dframe.reset_index(drop=True, inplace=True)\n",
    "    dframe.to_parquet(output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## metadata.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functions to load metadata information\n",
    "\"\"\"\n",
    "import logging\n",
    "from collections.abc import Iterable\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MAPPER = {\n",
    "    \"JCP2022_085227\": \"Aloxistatin\",\n",
    "    \"JCP2022_037716\": \"AMG900\",\n",
    "    \"JCP2022_025848\": \"Dexamethasone\",\n",
    "    \"JCP2022_046054\": \"FK-866\",\n",
    "    \"JCP2022_035095\": \"LY2109761\",\n",
    "    \"JCP2022_064022\": \"NVS-PAK1-1\",\n",
    "    \"JCP2022_050797\": \"Quinidine\",\n",
    "    \"JCP2022_012818\": \"TC-S-7004\",\n",
    "    \"JCP2022_033924\": \"DMSO\",\n",
    "    \"JCP2022_999999\": \"UNTREATED\",\n",
    "    \"JCP2022_UNKNOWN\": \"UNKNOWN\",\n",
    "    \"JCP2022_900001\": \"BAD CONSTRUCT\",\n",
    "}\n",
    "\n",
    "MICRO_CONFIG = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/jump-cellpainting/datasets/181fa0dc96b0d68511b437cf75a712ec782576aa/metadata/microscope_config.csv\"\n",
    ")\n",
    "MICRO_CONFIG[\"Metadata_Source\"] = \"source_\" + MICRO_CONFIG[\"Metadata_Source\"].astype(\n",
    "    str\n",
    ")\n",
    "MICRO_CONFIG = MICRO_CONFIG.set_index(\"Metadata_Source\")[\"Metadata_Microscope_Name\"]\n",
    "\n",
    "\n",
    "def find_feat_cols(cols: Iterable[str]):\n",
    "    \"\"\"Find column names for features\"\"\"\n",
    "    feat_cols = [c for c in cols if not c.startswith(\"Meta\")]\n",
    "    return feat_cols\n",
    "\n",
    "\n",
    "def find_meta_cols(cols: Iterable[str]):\n",
    "    \"\"\"Find column names for metadata\"\"\"\n",
    "    meta_cols = [c for c in cols if c.startswith(\"Meta\")]\n",
    "    return meta_cols\n",
    "\n",
    "\n",
    "def get_source_4_plate_redlist(plate_types: list[str]):\n",
    "    \"\"\"Get set of plate_id's  that should be not considered in the analysis\"\"\"\n",
    "    # https://github.com/jump-cellpainting/jump-orf-analysis/issues/1#issuecomment-921888625\n",
    "    # Low concentration plates\n",
    "    redlist = set([\"BR00127147\", \"BR00127148\", \"BR00127145\", \"BR00127146\"])\n",
    "    # https://github.com/jump-cellpainting/aws/issues/70#issuecomment-1182444836\n",
    "    redlist.add(\"BR00123528A\")\n",
    "\n",
    "    metadata = pd.read_csv(\"inputs/experiment-metadata.tsv\", sep=\"\\t\")\n",
    "    if \"ORF\" in plate_types:\n",
    "        # filter ORF plates.\n",
    "        query = 'Batch==\"Batch12\"'\n",
    "        bad_plates = set(metadata.query(query).Assay_Plate_Barcode)\n",
    "        redlist |= bad_plates\n",
    "\n",
    "    if \"TARGET2\" in plate_types:\n",
    "        # filter TARGET2 plates\n",
    "        query = 'Anomaly!=\"none\"'\n",
    "        bad_plates = set(metadata.query(query).Assay_Plate_Barcode)\n",
    "        redlist |= bad_plates\n",
    "    return redlist\n",
    "\n",
    "\n",
    "SOURCE3_BATCH_REDLIST = {\n",
    "    \"CP_32_all_Phenix1\",\n",
    "    \"CP_33_all_Phenix1\",\n",
    "    \"CP_34_mix_Phenix1\",\n",
    "    \"CP_35_all_Phenix1\",\n",
    "    \"CP_36_all_Phenix1\",\n",
    "    \"CP59\",\n",
    "    \"CP60\",\n",
    "}\n",
    "\n",
    "\n",
    "def build_path(row: pd.Series) -> str:\n",
    "    \"\"\"Create the path to the parquet file\"\"\"\n",
    "    template = (\n",
    "        \"./inputs/{Metadata_Source}/workspace/profiles/\"\n",
    "        \"{Metadata_Batch}/{Metadata_Plate}/{Metadata_Plate}.parquet\"\n",
    "    )\n",
    "    return template.format(**row.to_dict())\n",
    "\n",
    "\n",
    "def get_plate_metadata(sources: list[str], plate_types: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"Create filtered metadata DataFrame\"\"\"\n",
    "    plate_metadata = pd.read_csv(\"./inputs/metadata/plate.csv.gz\")\n",
    "    # Filter plates from source_4\n",
    "    if \"source_4\" in sources:\n",
    "        redlist = get_source_4_plate_redlist(plate_types)\n",
    "        plate_metadata = plate_metadata[~plate_metadata[\"Metadata_Plate\"].isin(redlist)]\n",
    "\n",
    "    # Filter plates from source_3 batches without DMSO\n",
    "    plate_metadata = plate_metadata[\n",
    "        (~plate_metadata[\"Metadata_Batch\"].isin(SOURCE3_BATCH_REDLIST))\n",
    "        | (plate_metadata[\"Metadata_PlateType\"] == \"TARGET2\")\n",
    "    ]\n",
    "\n",
    "    plate_metadata = plate_metadata[plate_metadata[\"Metadata_Source\"].isin(sources)]\n",
    "    plate_metadata = plate_metadata[\n",
    "        plate_metadata[\"Metadata_PlateType\"].isin(plate_types)\n",
    "    ]\n",
    "    return plate_metadata\n",
    "\n",
    "\n",
    "def get_well_metadata(plate_types: list[str]):\n",
    "    \"\"\"Load well metadata\"\"\"\n",
    "    well_metadata = pd.read_csv(\"./inputs/metadata/well.csv.gz\")\n",
    "    if \"ORF\" in plate_types:\n",
    "        orf_metadata = pd.read_csv(\"./inputs/metadata/orf.csv.gz\")\n",
    "        well_metadata = well_metadata.merge(orf_metadata, how=\"inner\")\n",
    "    # Use readable names for controls and non-treatment codes\n",
    "    well_metadata[\"Metadata_JCP2022\"] = well_metadata[\"Metadata_JCP2022\"].apply(\n",
    "        lambda x: MAPPER.get(x, x)\n",
    "    )\n",
    "    # Filter out wells\n",
    "    well_metadata = well_metadata[\n",
    "        ~well_metadata[\"Metadata_JCP2022\"].isin(\n",
    "            [\"UNTREATED\", \"UNKNOWN\", \"BAD CONSTRUCT\"]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return well_metadata\n",
    "\n",
    "\n",
    "def load_metadata(sources: list[str], plate_types: list[str]):\n",
    "    \"\"\"Load metadata only\"\"\"\n",
    "    plate = get_plate_metadata(sources, plate_types)\n",
    "    well = get_well_metadata(plate_types)\n",
    "    meta = well.merge(plate, on=[\"Metadata_Source\", \"Metadata_Plate\"])\n",
    "    return meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO    \u001b[0m Using column names from columns of adata.obsm\u001b[1m[\u001b[0m\u001b[32m'system'\u001b[0m\u001b[1m]\u001b[0m                                                   \n",
      "\u001b[34mINFO    \u001b[0m Using column names from columns of adata.obsm\u001b[1m[\u001b[0m\u001b[32m'covariates'\u001b[0m\u001b[1m]\u001b[0m                                               \n",
      "\u001b[34mINFO    \u001b[0m The model has been initialized                                                                            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 320/400:  80%|███████▉  | 319/400 [1:24:33<15:18, 11.34s/it, v_num=1, loss_train=-152]"
     ]
    }
   ],
   "source": [
    "batch_key = [\"Metadata_Batch\", \"Metadata_Plate\"]\n",
    "label_key = \"Metadata_JCP2022\",\n",
    "\n",
    "n_latent = 30\n",
    "\n",
    "adata = to_anndata(\"../outputs/scenario_1/mad_int_featselect.parquet\")\n",
    "meta = adata.obs.reset_index(drop=True).copy()\n",
    "\n",
    "if isinstance(batch_key, list):\n",
    "    actual_batch_key = batch_key[0]\n",
    "    categorical_covariate_keys = batch_key[1:]\n",
    "else:\n",
    "    batch_key = batch_key\n",
    "    categorical_covariate_keys = []\n",
    "\n",
    "SysVI.setup_anndata(adata, batch_key=actual_batch_key, categorical_covariate_keys=categorical_covariate_keys)\n",
    "vae = SysVI(adata, prior=\"standard_normal\")\n",
    "# vae.view_anndata_setup(adata=adata)\n",
    "vae.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    print(\"GPU is not available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_value = adata.X.min()\n",
    "# adata.X -= min_value\n",
    "\n",
    "if isinstance(batch_key, list):\n",
    "    actual_batch_key = batch_key[0]\n",
    "else:\n",
    "    categorical_covariate_keys = batch_key[1:]\n",
    "\n",
    "SysVI.setup_anndata(adata, batch_key=batch_key, categorical_covariate_keys=categorical_covariate_keys)\n",
    "vae = SysVI(adata, n_layers=2, n_latent=n_latent, prior=\"standard_normal\")\n",
    "\n",
    "\n",
    "vals = vae.get_latent_representation()\n",
    "features = [f'sysvi_{i}' for i in range(vals.shape[1])]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "batchcp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
